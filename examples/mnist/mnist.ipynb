{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_train.csv has been saved.\n",
      "mnist_test.csv has been saved.\n",
      "DataFrames loaded with MNIST data.\n",
      "Arrays loaded with MNIST data.\n"
     ]
    }
   ],
   "source": [
    "from mnist_utils import load_mnist\n",
    "import ml.array as ml\n",
    "\n",
    "# collect data\n",
    "train_images, train_labels, test_images, test_labels = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of train and test labels\n",
    "def one_hot_encode(data, classes):\n",
    "    out = []\n",
    "    for d in data:\n",
    "        one_hot = [0 for _ in range(classes)]\n",
    "        one_hot[int(d)] = 1\n",
    "        out.append(one_hot)\n",
    "    return out\n",
    "\n",
    "train_labels_onehot = ml.Array(one_hot_encode(train_labels.eval(), 10))\n",
    "test_labels_onehot = ml.Array(one_hot_encode(test_labels.eval(), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "x = ml.Array(train_images.eval()[:batch_size].tolist())\n",
    "y = ml.Array(train_labels_onehot.eval()[:batch_size].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model and computational graphs\n",
    "def softmax(x):\n",
    "    # exps = x.exp()\n",
    "    # return exps / exps.sum(1, keepdims=True)\n",
    "    return x / x.sum(1, keepdims=True)\n",
    "\n",
    "class Linear:\n",
    "\n",
    "    def __init__(self, inp_shape, out_shape):\n",
    "        self.w = ml.rand([inp_shape,out_shape])\n",
    "        self.b = ml.rand([out_shape])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.w + self.b.unsqueeze(0)\n",
    "\n",
    "l1 = Linear(784, 10)\n",
    "\n",
    "params = [l1.w, l1.b]\n",
    "\n",
    "pred = softmax(l1(x).relu())\n",
    "loss = -(y * pred.log()).sum()\n",
    "loss.build_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(73.49131, dtype=float32)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr, params):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "        self.temp_arrays = [loss.node.backend.empty(p.node.shape) for p in params]\n",
    "\n",
    "    def step(self):\n",
    "        for i, p in enumerate(params):\n",
    "            t = self.temp_arrays[i]\n",
    "            p.grad().set_reeval()\n",
    "            loss.node.backend.multiply(self.lr, p.grad().eval(), t)\n",
    "            loss.node.backend.negative(t, t)\n",
    "            loss.node.backend.add(p.node.data, t, p.node.data)\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr, params, beta1=0.9, beta2=0.999, epsilon=1e-8, backend=loss.node.backend):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.params = params\n",
    "        self.t = 0  # Time step\n",
    "        self.backend = backend  # The NumpyBackend instance\n",
    "\n",
    "        # Initialize moment vectors\n",
    "        self.m = [self.backend.empty(p.node.shape) for p in params]\n",
    "        self.v = [self.backend.empty(p.node.shape) for p in params]\n",
    "\n",
    "        # Initialize moments to zero\n",
    "        for i in range(len(self.params)):\n",
    "            self.backend.fill_inplace(self.m[i], 0)\n",
    "            self.backend.fill_inplace(self.v[i], 0)\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, p in enumerate(self.params):\n",
    "            grad = p.grad().eval()\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            self.backend.multiply(self.beta1, self.m[i], self.m[i])  # in-place scale by beta1\n",
    "            self.backend.multiply((1 - self.beta1), grad, grad)  # scale grad by (1-beta1)\n",
    "            self.backend.add(self.m[i], grad, self.m[i])  # in-place update of first moment\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.backend.multiply(self.beta2, self.v[i], self.v[i])  # in-place scale by beta2\n",
    "            self.backend.square(grad, grad)  # square the gradients in-place\n",
    "            self.backend.multiply((1 - self.beta2), grad, grad)  # scale squared grad by (1-beta2)\n",
    "            self.backend.add(self.v[i], grad, self.v[i])  # in-place update of second moment\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update parameters\n",
    "            self.backend.sqrt(v_hat, v_hat)  # in-place square root\n",
    "            self.backend.add(v_hat, self.epsilon, v_hat)  # in-place addition of epsilon\n",
    "            self.backend.reciprocal(v_hat, v_hat)  # in-place reciprocal\n",
    "            self.backend.multiply(m_hat, v_hat, v_hat)  # in-place multiplication with m_hat\n",
    "            self.backend.multiply(self.lr, v_hat, v_hat)  # in-place multiplication with learning rate\n",
    "            self.backend.add(p.node.data, -v_hat, p.node.data)  # update parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss =  array(75.9725, dtype=float32)\n",
      "\tloss =  array(75.9725, dtype=float32)\n",
      "\tloss =  array(75.9725, dtype=float32)\n",
      "\tloss =  array(75.9725, dtype=float32)\n",
      "\tloss =  array(75.9725, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "iter = 5000\n",
    "reports = 5\n",
    "\n",
    "# optim = SGD(lr, params)\n",
    "optim = Adam(lr, params)\n",
    "\n",
    "for it in range(iter):  \n",
    "    if (it % (iter//reports) == 0):\n",
    "        print(\"\\tloss = \", loss)\n",
    "    loss.eval()\n",
    "    loss.zero_grad()\n",
    "    loss.set_reeval()\n",
    "    optim.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
